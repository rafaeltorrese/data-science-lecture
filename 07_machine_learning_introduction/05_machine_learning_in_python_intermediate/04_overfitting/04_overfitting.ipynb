{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430da3a7",
   "metadata": {},
   "source": [
    "<center><h1>Overfitting</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439aa24",
   "metadata": {},
   "source": [
    "## 1. Introduction.\n",
    "\n",
    "\n",
    "<div><br><p>While exploring regression, we've briefly mentioned overfitting and the problems it can cause. We'll explore how to identify overfitting and what you can do to avoid it.  To explore overfitting, we'll use a dataset on cars which contains 7 numerical features that could have an effect on a car's fuel efficiency:</p>\n",
    "<ul>\n",
    "<li><code>cylinders</code> -- the number of <a href=\"http://bit.ly/29zLo9A\" target=\"_blank\">cylinders</a> in the engine.</li>\n",
    "<li><code>displacement</code> -- the engine's <a href=\"https://en.wikipedia.org/wiki/Engine_displacement\" target=\"_blank\">displacement</a></li>\n",
    "<li><code>horsepower</code> -- the engine's <a href=\"http://bit.ly/29sDwVU\" target=\"_blank\">horsepower</a></li>\n",
    "<li><code>weight</code> -- the car's weight.</li>\n",
    "<li><code>acceleration</code> -- the car's acceleration.</li>\n",
    "<li><code>model year</code> -- the year that car model was released (e.g. <code>70</code> corresponds to <code>1970</code>).</li>\n",
    "<li><code>origin</code> -- where the car was manufactured (<code>0</code> if North America, <code>1</code> if Europe, <code>2</code> if Asia).</li>\n",
    "</ul>\n",
    "<p>The <code>mpg</code> column is our target column and we want to predict using the other features.</p>\n",
    "<p>The dataset is hosted by the University of California Irvine on <a href=\"https://archive.ics.uci.edu/ml/datasets/Auto+MPG\" target=\"_blank\">their machine learning repository</a>.  You'll notice that the <strong>Data Folder</strong> contains multiple files.  We'll work with <code>auto-mpg.data</code>, which omits the 8 rows containing missing values for fuel efficiency (<code>mpg</code> column).</p>\n",
    "\n",
    "\n",
    "<h3>Exercise</h3>\n",
    "    \n",
    "<p>The starter code imports Pandas, reads the data into a dataframe, and cleans up some messy values.  Explore the dataset to become more familiar with it.</p>\n",
    "<p>If you run the code locally in Jupyter Notebook or Jupyter Lab, you'll notice a <a href=\"https://www.dataquest.io/blog/settingwithcopywarning/\" target=\"_blank\">SettingWithCopy Warning</a>. This won't prevent your code from running properly, but notifies you that whatever operation you're doing is trying to be set on a copy of a slice from a dataframe. To resolve this, it's considered good practice to include <code>.copy()</code> whenever you perform operations on a dataframe.</p></div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22917690",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"1. mpg:           continuous\",\n",
    "\"2. cylinders:     multi-valued discrete\",\n",
    "\"3. displacement:  continuous\",\n",
    "\"4. horsepower:    continuous\",\n",
    "\"5. weight:        continuous\",\n",
    "\"6. acceleration:  continuous\",\n",
    "\"7. model year:    multi-valued discrete\",\n",
    "\"8. origin:        multi-valued discrete\",\n",
    "\"9. car name:      string (unique for each instance)\",]\n",
    "\n",
    "columns = [name.strip().split(\":\")[0].split('.')[1].strip().replace(' ', '_') for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f06effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4d1716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horsepower    398 non-null    object \n",
      " 4   weight        398 non-null    float64\n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model_year    398 non-null    int64  \n",
      " 7   origin        398 non-null    int64  \n",
      " 8   car_name      398 non-null    object \n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 28.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Missing Attribute Values:  horsepower has 6 missing values\n",
    "cars = pd.read_table('auto-mpg.data', delim_whitespace=True, names=columns)\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4304beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['130.0' '165.0' '150.0' '140.0' '198.0' '220.0' '215.0' '225.0' '190.0'\n",
      " '170.0' '160.0' '95.00' '97.00' '85.00' '88.00' '46.00' '87.00' '90.00'\n",
      " '113.0' '200.0' '210.0' '193.0' '?' '100.0' '105.0' '175.0' '153.0'\n",
      " '180.0' '110.0' '72.00' '86.00' '70.00' '76.00' '65.00' '69.00' '60.00'\n",
      " '80.00' '54.00' '208.0' '155.0' '112.0' '92.00' '145.0' '137.0' '158.0'\n",
      " '167.0' '94.00' '107.0' '230.0' '49.00' '75.00' '91.00' '122.0' '67.00'\n",
      " '83.00' '78.00' '52.00' '61.00' '93.00' '148.0' '129.0' '96.00' '71.00'\n",
      " '98.00' '115.0' '53.00' '81.00' '79.00' '120.0' '152.0' '102.0' '108.0'\n",
      " '68.00' '58.00' '149.0' '89.00' '63.00' '48.00' '66.00' '139.0' '103.0'\n",
      " '125.0' '133.0' '138.0' '135.0' '142.0' '77.00' '62.00' '132.0' '84.00'\n",
      " '64.00' '74.00' '116.0' '82.00']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(cars['horsepower'].unique())\n",
    "print(\n",
    "    type(\n",
    "        cars['horsepower'].unique()\n",
    "    )\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771355d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cars = cars[cars['horsepower'] != '?'].copy()\n",
    "filtered_cars['horsepower'] = filtered_cars['horsepower'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c807a1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg             float64\n",
       "cylinders         int64\n",
       "displacement    float64\n",
       "horsepower      float64\n",
       "weight          float64\n",
       "acceleration    float64\n",
       "model_year        int64\n",
       "origin            int64\n",
       "car_name         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_cars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12767762",
   "metadata": {},
   "source": [
    "## 2. Bias and Variance.\n",
    "\n",
    "\n",
    "In order to understand overfitting, **bias** and **variance** must be understood as well. Bias and variance make up the two observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "Bias describes an error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that results in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides weight.\n",
    "\n",
    "Variance describes an error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, the result will be a low bias but high variance.\n",
    "\n",
    "In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f0b94",
   "metadata": {},
   "source": [
    "## 3. Bias-Variance Tradeoff.\n",
    "\n",
    "<div><br><p>We've discussed how overfitting generally happens when a model performs well on a training set, but doesn't generalize well with new data. We should think of overfitting as a relative term.  Between any two models, one will overfit more than the other one.  </p>\n",
    "<p>Understanding the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\">bias variance tradeoff</a> is critical to understanding overfitting.   Every process has some amount of natural noise that's unobservable.  Overfit models tend to capture the noise as well as the signal in a dataset.  </p>\n",
    "<p>Scott Fortman Roe's <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" target=\"_blank\">blog post on the bias-variance tradeoff</a> has a wonderful image that describes this tradeoff:</p>\n",
    "<p><img src=\"figs/2xqTu46.png\" alt=\"Imgur\"></p>\n",
    "<p>We can guess the bias of a model by training a few different models from the same class (linear regression in this case) using different features on the same dataset and calculating their error scores.  For regression, we can use mean absolute error, mean squared error, or R-squared.</p>\n",
    "<p>We can calculate the variance of the predicted values for each model we train leading to an increase in variance as we build more complex, multivariate models.</p>\n",
    "<p>While an extremely simple, univariate linear regression model underfits, an extremely complicated, multivariate linear regression model overfits.  Depending on the problem you're working on, there's a middle ground that helps construct reliable and useful predictive models.  </p>\n",
    "<p>Let's create a function that we can use for training the model and computing the bias and variance values. Then we can use the function to train some simple, univariate models.</p></div>\n",
    "\n",
    "### Exercise\n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "<p>Create a function named <code>train_and_test</code> that:</p>\n",
    "<ul>\n",
    "<li>Takes in a list of column names in <code>filtered_cars</code> as the sole parameter (<code>cols</code>),</li>\n",
    "<li>Trains a linear regression model using:<ul>\n",
    "<li>The columns in <code>cols</code> as the features,</li>\n",
    "<li>The <code>mpg</code> column as the target variable.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Uses the trained model to make predictions using the same input it was trained on,</li>\n",
    "<li>Computes the variance of the predicted values and the mean squared error between the predicted values and the actual label (<code>mpg</code> column).</li>\n",
    "<li>Returns the mean squared error value followed by the variance (e.g. <code>return(mse, variance)</code>).</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "<p>Use the <code>train_and_test</code> function to train a model using only the <code>cylinders</code> column.  Assign the resulting mean squared error value and variance to <code>cyl_mse</code> and <code>cyl_var</code>.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>Use the <code>train_and_test</code> function to train a model using only the <code>weight</code> column.  Assign the resulting mean squared error value and variance to <code>weight_mse</code> and <code>weight_var</code>.</p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8708cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(cols):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(filtered_cars[cols], filtered_cars['mpg'])\n",
    "    predicted = lr.predict(filtered_cars[cols])\n",
    "    variance = np.var(predicted)\n",
    "    mse = mean_squared_error(predicted, filtered_cars['mpg'])\n",
    "    return mse, variance\n",
    "\n",
    "cyl_mse, cyl_var = train_and_test(['cylinders'])\n",
    "weight_mse, weight_var = train_and_test(['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcc1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.02017956815553\n"
     ]
    }
   ],
   "source": [
    "print(cyl_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5c26e",
   "metadata": {},
   "source": [
    "## 4. Multivariate Models.\n",
    "\n",
    "Now that we have a function for training a regression model and calculating the mean squared error and variance, let's use it to train and understand more complex models.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "<p>Use the <code>train_and_test</code> function to train linear regression models using the following columns as the features:</p>\n",
    "<ul>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>.<ul>\n",
    "<li>MSE: <code>two_mse</code>, variance: <code>two_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>.<ul>\n",
    "<li>MSE: <code>three_mse</code>, variance: <code>three_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>.<ul>\n",
    "<li>MSE: <code>four_mse</code>, variance: <code>four_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code>.<ul>\n",
    "<li>MSE: <code>five_mse</code>, variance: <code>five_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code>, <code>model year</code><ul>\n",
    "<li>MSE: <code>six_mse</code>, variance: <code>six_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>columns: <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code>, <code>model year</code>, <code>origin</code><ul>\n",
    "<li>MSE: <code>seven_mse</code>, variance: <code>seven_var</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p>Use <code>print</code> statements or the variable inspector to display each value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941d6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_mse, two_var = train_and_test(['cylinders', 'displacement'])\n",
    "three_mse, three_var = train_and_test(['cylinders', 'displacement', 'horsepower'])\n",
    "four_mse, four_var = train_and_test(['cylinders', 'displacement', 'horsepower', 'weight'])\n",
    "five_mse, five_var = train_and_test(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'])\n",
    "six_mse, six_var = train_and_test(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year'])\n",
    "seven_mse, seven_var = train_and_test(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047e8b4",
   "metadata": {},
   "source": [
    "## 5. Cross Validation.\n",
    "\n",
    "The multivariate regression models you trained got better at reducing the amount of error.\n",
    "\n",
    "A good way to detect if your model is overfitting is to compare the **in-sample error** and the **out-of-sample error**, or the training error with the test error. So far, we calculated in the sample error by testing the model over the same training data. To calculate the out-of-sample error, we need to test the data on a test set of data. Unfortunately, we don't have a separate test dataset and need to use cross validation.\n",
    "\n",
    "If a model's cross validation error (out-of-sample error) is much higher than the in-sample error, then your data science senses should start to tingle. This is the first line of defense against overfitting and is a clear indicator that the trained model doesn't generalize well outside of the training set.\n",
    "\n",
    "Let's create a new function to handle performing the cross validation and computing the cross validation error.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "<div><br><p>Create a function named <code>train_and_cross_val</code> that:</p>\n",
    "<ul>\n",
    "<li>takes in a single parameter (list of column names),</li>\n",
    "<li>trains a linear regression model using the features specified in the parameter,</li>\n",
    "<li>uses the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\" target=\"_blank\">KFold</a> class to perform 10-fold validation using a random seed of 3 (we use this seed to answer check your code),</li>\n",
    "<li>calculates the mean squared error across all folds and the mean variance across all folds.</li>\n",
    "<li>returns the mean squared error value then the variance using a multiple return statement (e.g. <code>return(avg_mse, avg_var)</code>).</li>\n",
    "</ul>\n",
    "<p>Use the <code>train_and_cross_val</code> function to train linear regression models using the following columns as the features:</p>\n",
    "<ul>\n",
    "<li>the <code>cylinders</code> and <code>displacement</code> columns. Assign the resulting mean squared error value to <code>two_mse</code> and the resulting variance value to <code>two_var</code>.</li>\n",
    "<li>the <code>cylinders</code>, <code>displacement</code>, and <code>horsepower</code> columns.  Assign the resulting mean squared error value to <code>three_mse</code> and the resulting variance value to <code>three_var</code>.</li>\n",
    "<li>the <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, and <code>weight</code> columns. Assign the resulting mean squared error value to <code>four_mse</code> and the resulting variance value to <code>four_var</code>.</li>\n",
    "<li>the <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code> columns.  Assign the resulting mean squared error value to <code>five_mse</code> and the resulting variance value to <code>five_var</code>.</li>\n",
    "<li>the <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code>, and <code>model year</code> columns.  Assign the resulting mean squared error value to <code>six_mse</code> and the resulting variance value to <code>six_var</code>.</li>\n",
    "<li>the <code>cylinders</code>, <code>displacement</code>, <code>horsepower</code>, <code>weight</code>, <code>acceleration</code>, <code>model year</code>, and <code>origin</code> columns.  Assign the resulting mean squared error value to <code>seven_mse</code> and the resulting variance value to <code>seven_var</code>.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cada9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cross_val(cols):\n",
    "    variances = []\n",
    "    mses = []\n",
    "            \n",
    "    X = filtered_cars[cols]\n",
    "    y = filtered_cars['mpg']\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=3)    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        lr = LinearRegression()\n",
    "        \n",
    "        lr.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "        predictions = lr.predict(X.iloc[test_index])\n",
    "        \n",
    "        variance = np.var(predictions)\n",
    "        mse = mean_squared_error(predictions, y.iloc[test_index])\n",
    "        \n",
    "        variances.append(variance)\n",
    "        mses.append(mse)\n",
    "        \n",
    "    avg_var = np.mean(variances) \n",
    "    avg_mse = np.mean(mses)\n",
    "    \n",
    "    return (avg_mse, avg_var)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41849710",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_mse, two_var = train_and_cross_val(['cylinders', 'displacement'])\n",
    "three_mse, three_var = train_and_cross_val(['cylinders', 'displacement', 'horsepower'])\n",
    "four_mse, four_var = train_and_cross_val(['cylinders', 'displacement', 'horsepower', 'weight'])\n",
    "five_mse, five_var = train_and_cross_val(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration'])\n",
    "six_mse, six_var = train_and_cross_val(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year'])\n",
    "seven_mse, seven_var = train_and_cross_val(['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0ed4cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.584370274954374\n"
     ]
    }
   ],
   "source": [
    "print(two_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849ff7f",
   "metadata": {},
   "source": [
    "## 6. Plotting Cross-Validation Error vs. Cross-Validation Variance.\n",
    "\n",
    "During cross validation, the more features we add to the model, the lower the mean squared error becomes. This is a good sign and indicates that the model generalizes well to new data it wasn't trained on. As the mean squared error value went down, the variance of the predictions went up. This is expected, since the models with lower squared error values had higher model complexity, which tends to be more sensitive to small variations in input values (or high variance).\n",
    "\n",
    "For each model, let's plot the error and variance to get a better idea of the tradeoff as the number of features increased.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- On the same Axes instance:\n",
    "    - Generate a scatter plot with the model's number of features on the x-axis and the model's overall, cross-validation _mean squared error on the y-axis_. Use `red` for the scatter dot color.\n",
    "    - Generate a scatter plot with the model's number of features on the x-axis and the model's overall, cross-validation _variance on the y-axis_. Use `blue` for the scatter dot color.\n",
    "- Use `plt.show()` to display the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "994676cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*[*enumerate([two_mse, three_mse, four_mse, five_mse, six_mse, seven_mse], 2)]), color='red', label='Mean Squared Error')\n",
    "plt.scatter(range(2,8), [two_var, three_var, four_var, five_var, six_var, seven_var], color='blue', label='Variance')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Averages')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec5fa4",
   "metadata": {},
   "source": [
    "## 7. Conclusion.\n",
    "\n",
    "While the higher order multivariate models overfit in relation to the lower order multivariate models, the in-sample error and out-of-sample didn't deviate by much. The best model was around **50%** more accurate than the simplest model. On the other hand, the overall variance increased around **25%** as we increased the model complexity. This is a really good starting point, but your work is not done! The increased variance with the increased model complexity means that your model has more unpredictable performance on new, unseen data.\n",
    "\n",
    "If you were working on this problem on a data science team, you'd need to confirm the predictive accuracy of the model using completely new, unobserved data (e.g. maybe from cars from later years). Since you can't wait until a model is deployed in the wild to know how well it works, the exploration we did in this lesson helps approximate a model's real-world performance.\n",
    "\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3326f5",
   "metadata": {},
   "source": [
    "## 8. Next Steps.\n",
    "\n",
    "In this lesson, we explored overfitting at a deeper level and introduced related terminology that you'll see in other literature as well. So far, we've mostly dealt with supervised machine learning models to solve regression and classification problems. In the next lesson, we'll explore an **unsupervised machine learning** technique called k-means clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
