{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3d3a2a",
   "metadata": {},
   "source": [
    "<center><h1>Descenso de gradiente</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1374a",
   "metadata": {},
   "source": [
    "## 1. Introducción.\n",
    "El modelo de regresión lineal estima la relación entre las características y la columna objetivo y cómo podemos usar eso para hacer predicciones. A continuación, nalizaremos las dos formas más comunes de encontrar los valores óptimos de los parámetros para un modelo de regresión lineal. Cada combinación de valores de parámetros únicos forma un modelo de regresión lineal único, y el proceso de encontrar estos valores óptimos se conoce como **ajuste del modelo**. En ambos enfoques del ajuste de modelos, intentaremos minimizar la siguiente función:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} ({\\hat{y_i} - y_i})^2$$\n",
    "\n",
    "Esta función es el error cuadrático medio entre las etiquetas predichas hechas usando un modelo dado y las etiquetas verdaderas. El problema de elegir un conjunto de valores que minimicen o maximicen otra función se conoce como [problema de optimización](https://en.wikipedia.org/wiki/Mathematical_optimization).\n",
    "\n",
    "Para desarrollar la intuición para el proceso de optimización, comencemos con un modelo de regresión lineal de un solo parámetro:\n",
    "\n",
    "$\\hat{y} = a_1x_1$\n",
    "\n",
    "Tenga en cuenta que esto es diferente de un modelo de regresión lineal simple, que en realidad tiene dos parámetros $a_0$ y $a_1$\n",
    " \n",
    "$$\\hat{y} = a_1x_1 + a_0$$\n",
    "\n",
    "\n",
    "Usemos la columna `Gr Liv Area` para el parámetro único:\n",
    "\n",
    "$\\hat{SalePrice} = a_1 * Gr Liv Area$\n",
    "\n",
    "\n",
    "<img src=\"figs/single_var_operation.gif\" height=\"300\" width=\"500\" alt=\"optimization\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a572fb",
   "metadata": {},
   "source": [
    "## 2. Descenso de gradiente de una sola variable.\n",
    "\n",
    "En la imagen anterior bservamos como la función de optimización sigue una curva con un valor mínimo. Esto debería recordar la exploración de los valores mínimos relativos en cálculo. Calculamos los puntos críticos calculando la derivada de la curva, estableciéndola igual a $0$, y encontrando el valor $x$ en este punto. \n",
    "\n",
    "Desafortunadamente, este enfoque no funcionará cuando tengamos varios valores de parámetros porque minimizar el valor de un parámetro puede aumentar el valor de otro parámetro. Además, si bien podemos trazar la curva MSE cuando solo tenemos un único parámetro, estamos tratando de encontrar y seleccionar visualmente el valor que minimiza el MSE, este enfoque no funcionará cuando tenemos varios valores de parámetros porque no podemos visualizar 3 dimensiones pasadas.\n",
    "\n",
    "Exploraremos una técnica iterativa para resolver este problema, conocida como **descenso de gradiente**. El [algoritmo de descenso de gradiente](https://en.wikipedia.org/wiki/Gradient_descent) funciona probando iterativamente diferentes valores de parámetros hasta que se encuentra el modelo con el error cuadrático medio más bajo. El descenso de gradiente es una técnica de optimización de uso común también para otros modelos, como las redes neuronales.\n",
    "\n",
    "Aquí hay una descripción general del algoritmo de descenso de gradiente para un modelo de regresión lineal de un solo parámetro:\n",
    "\n",
    "- seleccione valores iniciales para el parámetro: $a_1$\n",
    "- repita hasta la convergencia (generalmente implementada con un número máximo de iteraciones):\n",
    "    - calcule el error (MSE) del modelo que utiliza el valor del parámetro actual: $MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}^{(i)} - y^{(i)} ) ^2$\n",
    "    - calcule la derivada del error (MSE) en el valor actual del parámetro: $\\frac{d}{da_1} MSE(a_1)$\n",
    "    - actualice el valor del parámetro restando la derivada por una constante ($\\alpha$, llamada tasa de aprendizaje): $a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "\n",
    "En el último paso del algoritmo, notará que usamos `:=` para indicar que el valor de la derecha se asigna a la variable de la izquierda.\n",
    "    \n",
    "Mientras que en Python, hemos usado el operador de igualdad (`=`) para la asignación, lo hemos usado en matemáticas (`=`) para significar igualdad. Por ejemplo, `a = 1` en Python asigna el valor `1` a la variable `a`. En matemáticas, `a = 1` afirma que `a` es igual a `1`. En artículos matemáticos, a veces también se usa $\\leftarrow$ para asignación: $a_1 \\leftarrow a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "\n",
    "La selección de un parámetro inicial y una tasa de aprendizaje adecuados reducirá el número de iteraciones necesarias para converger y forma parte de la optimización de hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e29e5",
   "metadata": {},
   "source": [
    "## 3. Derivada de la función de costo.\n",
    "\n",
    "En optimización matemática, una función que optimizamos a través de la minimización se conoce como función de costo o, a veces, como función de pérdida. Debido a que estamos tratando de ajustar un modelo de parámetro único, podemos reemplazar $\\hat{y}^{(i)}$ con $a_1x_1^{(i)}$ en la función de costo:\n",
    "\n",
    "$$MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} (a_1x_1^{(i)} - y^{(i)} ) ^2$$\n",
    "\n",
    "Aplicaremos propiedades de cálculo para simplificar esta derivada a algo que podamos calcular. Se sugiere que sigas con lápiz y papel, y veas si puedes aplicar las propiedades que mencionamos en cada paso para obtener el mismo resultado. Tenga en cuenta que si bien probablemente nunca tendrá que implementar el descenso de gradiente usted mismo (ya que la mayoría de los paquetes tienen implementaciones de alto rendimiento), comprender las matemáticas le ayudará a depurar más fácilmente cuando tenga problemas.\n",
    "\n",
    "$$\\frac{d}{da_1} MSE(a_1) = \\frac{d}{da_1} \\frac{1}{n} \\sum_{i=1}^{n} (a_1x_1^{(i)} - y^{(i)} ) ^2$$\n",
    "\n",
    "Al aplicar la propiedad de [diferenciación lineal](https://en.wikipedia.org/wiki/Linearity_of_differentiation) del cálculo, podemos traer el término derivado dentro de la sumatoria:\n",
    "\n",
    "$$\\frac{d}{da_1} MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{da_1} (a_1x_1^{(i)} - y^{(i)} ) ^2$$\n",
    "\n",
    "Podemos aplicar tanto la regla de la potencia como la regla de la cadena para simplificar esto:\n",
    "\n",
    "$$\\frac{d}{da_1} MSE(a_1) = \\frac{1}{n} \\sum_{i=1}^{n} 2(a_1x_1^{(i)} - y^{(i)})  \\frac{d}{da_1} (a_1x_1^{(i)} - y^{(i)} )$$\n",
    "\n",
    "Porque estamos diferenciando $a_1x_1^{(i)} - y^{(i)}$ con respecto a $a_1$, tratamos a $y^{(i)}$ y $x_1^{(i)}$ como constantes $y^{(i)}$ y luego se simplifica a solo $x_1^{(i)}$: \n",
    "\n",
    "$$\\frac{d}{da_1} MSE(a_1) = \\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}(a_1x_1^{(i)} - y^{(i)})$$\n",
    "\n",
    "Para cada iteración de descenso de gradiente:\n",
    "\n",
    "- la derivada se calcula usando el valor actual de $a_1$\n",
    "- la derivada es multiplicada por la tasa de aprendizaje ($\\alpha$): $\\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "- El resultado se resta del valor del parámetro actual y se asigna como el nuevo valor del parámetro: $a_1 := a_1 - \\alpha \\frac{d}{da_1} MSE(a_1)$\n",
    "\n",
    "\n",
    "Así es como se vería esto en el código si ejecutamos el descenso de gradiente durante `10` iteraciones:\n",
    "\n",
    "```python\n",
    "a1_list = [1000]\n",
    "alpha = 10\n",
    "\n",
    "for x in range(10):\n",
    "    a1 = a1_list[x]\n",
    "    deriv = derivative(a1, alpha, xi_list, yi_list)\n",
    "    a1_new = a1 - alpha*deriv\n",
    "    a1_list.append(a1_new)\n",
    "```\n",
    "\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Termine de implementar la función `derivative()`:\n",
    "    - Esta función debe devolver la derivada en el valor actual de $a_1$.\n",
    "- Descomente las 2 líneas de código que ejecutan la función `gradient_descent()`, asigne la lista de iteraciones para $a_1$  a `param_iterations`, y asigne la última iteración para $a_1$ a `final_param`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b2c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e84c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmesHousing.txt', delimiter='\\t')\n",
    "train = data[:1460].copy()\n",
    "test = data[1460:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37397a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(a1, xi_list, yi_list):\n",
    "    assert len(xi_list) == len(yi_list), 'x and y lists are different in length'\n",
    "    n = len(xi_list)\n",
    "    return (2 / n )*sum(xi * (a1 * xi - yi) for xi,yi in zip(xi_list, yi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f074d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial):\n",
    "    a1_list = [a1_initial]\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        deriv = derivative(a1, xi_list, yi_list)\n",
    "        a1_new = a1 - alpha * deriv\n",
    "        a1_list.append(a1_new)\n",
    "    return a1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b306f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.14219147202738\n"
     ]
    }
   ],
   "source": [
    "# quitar comentario cuando esté lista la función.\n",
    "param_iterations = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150)\n",
    "final_param = param_iterations[-1]\n",
    "print(final_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322a9e9",
   "metadata": {},
   "source": [
    "## 4. Comprensión del descenso de gradiente multiparámetro.\n",
    "\n",
    "Ahora que hemos entendido cómo funciona el descenso de gradiente de un solo parámetro, desarrollemos algo de intuición para el descenso de gradiente de múltiples parámetros. Comencemos visualizando el MSE como una función de los valores de los parámetros para el siguiente modelo de regresión lineal simple:\n",
    "\n",
    "$$SalePrice = a_1 * Gr Liv Area + a_0$$\n",
    "\n",
    "En la imagen de abajo, hay un diagrama de dispersión 3D con:\n",
    "\n",
    "- $a_0$ en el eje x\n",
    "- $a_1$ en el eje y\n",
    "- MSE en el eje z\n",
    "\n",
    "<img src=\"figs/surface_plot.gif\" height=\"400\" width=\"600\" alt=\"optimization\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15154ecc",
   "metadata": {},
   "source": [
    "## 5. Gradiente de la función de costo.\n",
    "\n",
    "Intentamos reducir la suma residual de cuadrados (que, por poder, también reduce el error cuadrático medio). El [gradiente](https://en.wikipedia.org/wiki/Gradient) es una generalización multivariable de la derivada. Anteriormente, nos preocupamos por minimizar la siguiente función de costo:\n",
    "\n",
    "$\\displaystyle  MSE(a_1) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\left(a_1x_1^{(i)} - y^{(i)}\\right) ^2$\n",
    "\n",
    "Cuando tenemos 2 valores de parámetro ($a_0$ y $a_1$), la función de costo ahora es una función de dos variables, no de una:\n",
    "\n",
    "$\\displaystyle  MSE(a_0, a_1) = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} \\left(a_0 + a_1x_1^{(i)} - y^{(i)} \\right) ^2$\n",
    "\n",
    "En lugar de una regla de actualización, ahora necesitamos dos reglas de actualización. Necesitamos uno para $a_0$:\n",
    "\n",
    "$\\displaystyle a_0 := a_0 - \\alpha \\dfrac{d}{da_0} MSE(a_0, a_1)$\n",
    "\n",
    "y otra para $a_1$:\n",
    "\n",
    "$a_1 := a_1 - \\alpha \\dfrac{d}{da_1} MSE(a_0, a_1)$\n",
    "\n",
    "Determinamos que $\\frac{d}{da_1} MSE(a_1)$ nos dio $\\displaystyle \\frac{2}{n} \\sum\\limits_{i=1}^{n} x_1^{(i)}\\left(a_1x_1^{(i)} - y^{(i)}\\right)$. Para el caso de multiparámetro, necesitamos incluir el parámetro adicional:\n",
    "\n",
    "$\\displaystyle  \\dfrac{d}{da_1} MSE(a_0, a_1) = \\frac{2}{n} \\sum\\limits_{i=1}^{n} x_1^{(i)}\\left(a_0 + a_1x_1^{(i)} - y^{(i)}\\right)$\n",
    "\n",
    "Para $\\frac{d}{da_0} MSE(a_0, a_1)$ no revisaremos la prueba de esta derivada, pero es similar a la que hicimos para\n",
    "$a_1$:\n",
    "\n",
    "\n",
    "$\\displaystyle  \\dfrac{d}{da_0} MSE(a_0, a_1) = \\frac{2}{n} \\sum\\limits_{i=1}^{n} \\left(a_0 + a_1x_1^{(i)} - y^{(i)}\\right)$\n",
    "\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Implemente la función `a0_derivative()`, que implementa el gradiente para $a_0$.\n",
    "    - Aunque estamos trabajando en el caso de varios parámetros, mantengamos este nombre de función consistente con el anterior que implementamos (`a1_derivative()`).\n",
    "    - Agregamos el parámetro `a0` a los parámetros de la función. Esto se debe a que necesitamos ambos parámetros para las actualizaciones de parámetros individuales (verifique esto observando las operaciones matemáticas que exploramos anteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873b5b44",
   "metadata": {},
   "outputs": [],
   "source": [
    " def a0_derivative(a0, a1, xi_list, yi_list):\n",
    "    assert len(xi_list) == len(yi_list), 'x and y lists are different in length'\n",
    "    n = len(xi_list)\n",
    "    return (2 / n )*sum(a0 + a1 * xi - yi for xi,yi in zip(xi_list, yi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdc6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a1_derivative(a0, a1, xi_list, yi_list):\n",
    "    assert len(xi_list) == len(yi_list), 'x and y lists are different in length'\n",
    "    n = len(xi_list)\n",
    "    return (2 / n )*sum(xi * (a0 + a1 * xi - yi) for xi,yi in zip(xi_list, yi_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe13b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(xi_list, yi_list, max_iterations, alpha, a1_initial, a0_initial):\n",
    "    a1_list = [a1_initial]\n",
    "    a0_list = [a0_initial]\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        a1 = a1_list[i]\n",
    "        a0 = a0_list[i]\n",
    "        \n",
    "        a1_deriv = a1_derivative(a0, a1, xi_list, yi_list)\n",
    "        a0_deriv = a0_derivative(a0, a1, xi_list, yi_list)\n",
    "        \n",
    "        a1_new = a1 - alpha * a1_deriv\n",
    "        a0_new = a0 - alpha * a0_deriv\n",
    "        \n",
    "        a1_list.append(a1_new)\n",
    "        a0_list.append(a0_new)\n",
    "        \n",
    "    return a0_list, a1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6181867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 999.9729797812329, 999.985903701066, 999.980232547139, 999.9832179015052, 999.9821734177915, 999.983004932363, 999.9829631191217, 999.9833278635107, 999.98350334434, 999.9837669324418, 999.9839895042135, 999.9842311701743, 999.9844639472566, 999.9847008623329, 999.9849358510428, 999.9851717365096, 999.9854072044933, 999.985642866808, 999.9858784386378, 999.986114052572]\n"
     ]
    }
   ],
   "source": [
    "# quitar comentario cuando esté lista la función.\n",
    "a0_params, a1_params = gradient_descent(train['Gr Liv Area'], train['SalePrice'], 20, .0000003, 150, 1000)\n",
    "print(a0_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab8a5d",
   "metadata": {},
   "source": [
    "## 6. Descenso de gradiente para dimensiones más altas.\n",
    "¿Qué pasa si queremos usar muchos parámetros en nuestro modelo? El descenso de gradiente en realidad se escala a tantas variables como desee. Cada valor de parámetro necesitará su propia regla de actualización, y coincide estrechamente con la regla de actualización para $a_1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\displaystyle  a_0 :=& a_0 - \\alpha \\dfrac{d}{da_0} MSE \\\\\n",
    " a_1 := & a_1 - \\alpha \\dfrac{d}{da_1} MSE \\\\ \n",
    " a_2 := & a_2 - \\alpha \\dfrac{d}{da_2} MSE \\\\ \n",
    " \\vdots\\\\\n",
    " a_n := & a_n - \\alpha \\dfrac{d}{da_n} MSE \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Además de la derivada para el MSE con respecto al valor de la intercepción ($a_0$), las derivadas para otros parámetros son idénticas:\n",
    "\n",
    "\\begin{align*}\n",
    "\\displaystyle  \\dfrac{d}{da_1} MSE = &\\frac{2}{n} \\sum_{i=1}^{n} x_1^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\\\  \n",
    "\\displaystyle \\dfrac{d}{da_2} MSE =  &\\dfrac{2}{n} \\sum_{i=1}^{n} x_2^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\\\\n",
    " \\vdots\\\\\n",
    "\\displaystyle \\dfrac{d}{da_n} MSE =  &\\dfrac{2}{n} \\sum_{i=1}^{n} x_n^{(i)}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)  \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8a9a7",
   "metadata": {},
   "source": [
    "## 7. Conclusión.\n",
    "En esta lección, exploramos cómo encontrar un modelo de regresión lineal utilizando el algoritmo de descenso de gradiente. Los principales desafíos con el descenso de gradiente incluyen:\n",
    "\n",
    "- Elegir buenos valores de parámetros iniciales\n",
    "- Elegir una buena tasa de aprendizaje (cae bajo el dominio de la optimización de hiperparámetros)\n",
    "- En la próxima lección, exploraremos una técnica llamada estimación OLS (Mínimos cuadrados ordinarios) que no requiere ninguna selección de valores de parámetros o hiperparámetros.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
