{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b983a9",
   "metadata": {},
   "source": [
    "<center><h1>Evaluating Model Performance</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09868201",
   "metadata": {},
   "source": [
    "## 1. Testing quality of predictions\n",
    "\n",
    "We now have a function that can predict the price for any living space we want to list as long as we know the number of people it can accommodate. The function we wrote represents a **machine learning model**, which means that it outputs a prediction based on the input to the model.\n",
    "\n",
    "A simple way to test the quality of your model is to:\n",
    "\n",
    "- split the dataset into 2 partitions:\n",
    "    - the training set: contains the majority of the rows (75%)\n",
    "    - the test set: contains the remaining minority of the rows (25%)\n",
    "\n",
    "- use the rows in the training set to predict the price value for the rows in the test set\n",
    "    - add new column named predicted_price to the test set\n",
    "- compare the `predicted_price` values with the actual price values in the test set to see how accurate the predicted values were.\n",
    "\n",
    "This validation process, where we use the training set to make predictions and the test set to predict values for, is known as **train/test validation**. Whenever you're performing machine learning, you want to perform validation of some kind to ensure that your machine learning model can make good predictions on new data. While train/test validation isn't perfect, we'll use it to understand the validation process, to select an error metric, and then we'll dive into a more robust validation process later in this course.\n",
    "\n",
    "Let's modify the `predict_price` function to use only the rows in the training set, instead of the full dataset, to find the nearest neighbors, average the `price` values for those rows, and return the predicted price value. Then, we'll use this function to predict the price for just the rows in the test set. Once we have the predicted price values, we can compare with the true price values and start to understand the model's effectiveness in the next screen.\n",
    "\n",
    "To start, we've gone ahead and assigned the first 75% of the rows in `dc_listings` to `train_df` and the last 25% of the rows to `test_df`. Here's a diagram explaining the split:\n",
    "\n",
    "<img src=\"figs/train_test_split.png\" width=\"800\" heigth=\"600\"/>\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Within the `predict_price` function, change the Dataframe that `temp_df` is assigned to. Change it from `dc_listings` to `train_df`, so only the training set is used.\n",
    "- Use the Series method `apply` to pass all of the values in the `accommodates` column from `test_df` through the `predict_price` function.\n",
    "- Assign the resulting Series object to the `predicted_price` column in `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd60d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60cb3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings = pd.read_csv('dc_airbnb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings['price'] = dc_listings['price'].str.replace('[\\$,]', '').astype(float) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6563517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dc_listings.iloc[:2792]\n",
    "test_df = dc_listings.iloc[2792:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad91e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(new_listing):\n",
    "    temp_df = dc_listings.copy()\n",
    "    temp_df['distance'] = (temp_df['accommodates'] - new_listing).abs()\n",
    "    return temp_df.sort_values(by='distance')['price'].head().mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c350a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise\n",
    "def predict_price(new_listing):\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['distance'] = (temp_df['accommodates'] - new_listing).abs()\n",
    "    return temp_df.sort_values(by='distance')['price'].head().mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a23f24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAFAEL~1.TOR\\AppData\\Local\\Temp/ipykernel_6592/1789143025.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['predicted_price'] = test_df['accommodates'].apply(predict_price)\n"
     ]
    }
   ],
   "source": [
    "test_df['predicted_price'] = test_df['accommodates'].apply(predict_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b0af8",
   "metadata": {},
   "source": [
    "## 2. Error Metrics\n",
    "\n",
    "We now need a metric that quantifies how good the predictions were on the test set. This class of metrics is called an error metric. As the name suggests, an error metric quantifies how inaccurate our predictions were compared to the actual values. In our case, the error metric tells us how off our predicted price values were from the actual price values for the living spaces in the test dataset.\n",
    "\n",
    "We could start by calculating the difference between each predicted and actual value and then averaging these differences. This is referred to as mean error but isn't an effective error metric for most cases. Mean error treats a positive difference differently than a negative difference, but we're really interested in how far off the prediction is in either the positive or negative direction. If the true price was 200 dollars and the model predicted 210 or 190 it's off by 10 dollars either way.\n",
    "\n",
    "We can instead use the mean absolute error, where we compute the absolute value of each error before we average all the errors.\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{k=1}^{n} |actual_1 - predicted_1| + \\cdots + |actual_n - predicted_n|$$\n",
    "\n",
    "### Exercise\n",
    "- Use `numpy.absolute()` to calculate the mean absolute error between `predicted_price` and `price`.\n",
    "- Assign the MAE to `mae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37370a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.29001074113876\n"
     ]
    }
   ],
   "source": [
    "mae = (test_df['predicted_price'] - test_df['price']).abs().mean()\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced487d2",
   "metadata": {},
   "source": [
    "## 3. Mean Squared Error\n",
    "\n",
    "For many prediction tasks, we want to penalize predicted values that are further away from the actual value far more than those closer to the actual value.\n",
    "\n",
    "We can instead take the mean of the squared error values, which is called the **mean squared error** or MSE for short. The MSE makes the gap between the predicted and actual values more clear. A prediction that's off by 100 dollars will have an error (of 10,000) that's 100 times more than a prediction that's off by only 10 dollars (which will have an error of 100).\n",
    "\n",
    "Here's the formula for MSE:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{k=1}^{n} (actual_1 - predicted_1)^2 + \\cdots + (actual_n - predicted_n)^2$$\n",
    "\n",
    "where `n` represents the number of rows in the test set. Let's calculate the MSE value for the predictions we made on the test set.\n",
    "\n",
    "### Exercise\n",
    "- Calculate the MSE value between the `predicted_price` and price columns and assign to `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e758fbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18646.525370569325\n"
     ]
    }
   ],
   "source": [
    "mse = np.power(test_df['predicted_price']- test_df['price'], 2).mean()\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c7c48",
   "metadata": {},
   "source": [
    "## 4. Training another model\n",
    "The model we trained achieved a mean squared error of around **18646.5**. Is this a high or a low mean squared error value? What does this tell us about the quality of the predictions and the model? By itself, the mean squared error value for a single model isn't all that useful.\n",
    "\n",
    "The units of mean squared error in our case is dollars squared (not dollars), which makes it hard to reason about intuitively as well. We can, however, train another model and then compare the mean squared error values to see which model performs better on a relative basis. Recall that a low error metric means that the gap between the predicted list price and actual list price values is low while a high error metric means the gap is high.\n",
    "\n",
    "Let's train another model, this time using the `bathrooms` column, and compare MSE values.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Modify the `predict_price` function to the right to use the `bathrooms` column instead of the `accommodates` column to make predictions.\n",
    "- Apply the function to test_df and assign the resulting Series object containing the predicted price values to the predicted_price column in test_df.\n",
    "- Calculate the squared error between the price and predicted_price columns in test_df and assign the resulting Series object to the squared_error column in test_df.\n",
    "- Calculate the mean of the squared_error column in test_df and assign to mse.\n",
    "- Use the print function or the variables inspector to display the MSE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "587f7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise\n",
    "def predict_price(new_listing):\n",
    "    temp_df = train_df.copy()\n",
    "    temp_df['distance'] = (temp_df['bathrooms'] - new_listing).abs()\n",
    "    return temp_df.sort_values(by='distance')['price'].head().mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da8f8e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAFAEL~1.TOR\\AppData\\Local\\Temp/ipykernel_6592/1150879193.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['predicted_price'] = test_df['bathrooms'].apply(predict_price)\n"
     ]
    }
   ],
   "source": [
    "test_df['predicted_price'] = test_df['bathrooms'].apply(predict_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eb84ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18405.444081632548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAFAEL~1.TOR\\AppData\\Local\\Temp/ipykernel_6592/2349769556.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['squared_error'] = (test_df['price'] - test_df['predicted_price'])**2\n"
     ]
    }
   ],
   "source": [
    "test_df['squared_error'] = (test_df['price'] - test_df['predicted_price'])**2\n",
    "mse = test_df['squared_error'].mean()\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dea65f",
   "metadata": {},
   "source": [
    "## 5. Root Mean Squared Error\n",
    "While comparing MSE values helps us identify which model performs better on a relative basis, it doesn't help us understand if the performance is good enough in general. This is because the units of the MSE metric are squared (in this case, dollars squared). An MSE value of 16377.5 dollars squared doesn't give us an intuitive sense of how far off the model's predictions are from the true price value in dollars.\n",
    "\n",
    "**Root mean squared error** is an error metric whose units are the base unit (in our case, dollars). RMSE for short, this error metric is calculated by taking the square root of the MSE value:\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "Since the RMSE value uses the same units as the target column, we can understand how far off in real dollars we can expect the model to perform.\n",
    "\n",
    "Let's calculate the RMSE value of the model we trained using the `bathrooms\n",
    "` column.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Calculate the RMSE value of the model we trained using the bathrooms column and assign it to `rmse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a875fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.6666653295221\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5139d",
   "metadata": {},
   "source": [
    "## 6. Comparing MAE and RMSE\n",
    "The model achieved an RMSE value of approximately 135.6, which implies that we should expect for the model to be off by 135.6 dollars on average for the predicted price values. Given that most of the living spaces are listed at just a few hundred dollars, we need to reduce this error as much as possible to improve the model's usefulness.\n",
    "\n",
    "We discussed a few different error metrics we can use to understand a model's performance. As we mentioned earlier, these individual error metrics are helpful for comparing models. To better understand a specific model, we can compare multiple error metrics for the same model. This requires a better understanding of the mathematical properties of the error metrics.\n",
    "\n",
    "If you look at the equation for MAE:\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{k=1}^{n} |actual_1 - predicted_1| + \\cdots + |actual_n - predicted_n|$$\n",
    "\n",
    "\n",
    "you'll notice that that the differences between predicted and actual values grow linearly. A prediction that's off by 10 dollars has a 10 times higher error than a prediction that's off by 1 dollar. If you look at the equation for RMSE, however:\n",
    "\n",
    "$$RMAE = \\sqrt{\\frac{\\sum_{k=1}^{n} (actual_1 - predicted_1)^2 + \\cdots + (actual_n - predicted_n)^2}{n}}$$ \n",
    "\n",
    "\n",
    "you'll notice that each error is squared before the square root of the sum of all the errors is taken. This means that the individual errors grows quadratically and has a different effect on the final RMSE value.\n",
    "\n",
    "Let's look at an example using different data entirely. We've created 2 Series objects containing 2 sets of errors and assigned to `errors_one` and `errors_two`.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Calculate the MAE for `errors_one` and assign to `mae_one`.\n",
    "- Calculate the RMSE for `errors_one` and assign to `rmse_one`.\n",
    "- Calculate the MAE for `errors_two` and assign to `mae_two`.\n",
    "- Calculate the RMSE for `errors_two` and assign to `rmse_two`.\n",
    "\n",
    "```python\n",
    "errors_one = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10])\n",
    "errors_two = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 1000])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b8f3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_one = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10])\n",
    "errors_two = pd.Series([5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 10, 5, 1000])\n",
    "\n",
    "mae_one = errors_one.mean()\n",
    "rmse_one = np.sqrt(np.power(errors_one, 2).mean())\n",
    "\n",
    "mae_two = errors_two.mean()\n",
    "rmse_two = np.sqrt(np.power(errors_two, 2).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
