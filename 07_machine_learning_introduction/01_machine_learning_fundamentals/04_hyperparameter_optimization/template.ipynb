{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1590b6bb",
   "metadata": {},
   "source": [
    "<center><h1>Hyperparameter Optimization</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a03091",
   "metadata": {},
   "source": [
    "## 1. Recap.\n",
    "\n",
    "We focused on increasing the number of attributes the model uses. We saw how, in general, adding more attributes generally lowered the error of the model. This is because the model is able to do a better job identifying the living spaces from the training set that are the most similar to the ones from the test set. However, we also observed how using all of the available features didn't actually improve the model's accuracy automatically and that some of the features were probably not relevant for similarity ranking. We learned that selecting relevant features was the right lever when improving a model's accuracy, not just increasing the features used in the absolute.\n",
    "\n",
    "In this lesson, we'll focus on the impact of increasing `k`, the number of nearby neighbors the model uses to make predictions. We exported both the training (`train_df`) and test sets (`test_df`) from the last lessons to CSV files, `dc_airbnb_train.csv` and `dc_airbnb_test.csv` respectively. Let's read both these CSV's into Dataframes.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Read `dc_airbnb_train.csv` into a Dataframe and assign to `train_df`.\n",
    "- Read `dc_airbnb_test.csv` into a Dataframe and assign to `test_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d38e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d51aa0f",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter optimization.\n",
    "\n",
    "When we vary the features that are used in the model, we're affecting the data that the model uses. On the other hand, varying the k value affects the behavior of the model independently of the actual data that's used when making predictions. In other words, we're impacting how the model performs without trying to change the data that's used.\n",
    "\n",
    "Values that affect the behavior and performance of a model that are unrelated to the data that's used are referred to as **hyperparameters**. The process of finding the optimal hyperparameter value is known as [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization). A simple but common hyperparameter optimization technique is known as [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search), which involves:\n",
    "\n",
    "selecting a subset of the possible hyperparameter values,\n",
    "training a model using each of these hyperparameter values,\n",
    "evaluating each model's performance,\n",
    "selecting the hyperparameter value that resulted in the lowest error value.\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. While grid search can take a long time when working with large datasets, the data we're working with in this lesson is small and this process is relatively quick.\n",
    "\n",
    "Let's confirm that grid search will work quickly for the dataset we're working with by first observing how the model performance changes as we increase the k value from `1` to `5`. If you recall, we set `5` as the k value for the last 2 lessons. Let's use the features from the last lesson that resulted in the best model accuracy:\n",
    "\n",
    "- `accommodates`\n",
    "- `bedrooms`\n",
    "- `bathrooms`\n",
    "- `number_of_reviews`\n",
    "\n",
    "### Exercise\n",
    "\n",
    "\n",
    "- Create a list containing the integer values `1`, `2`, `3`,`4`,  and `5`, in that order, and assign to `hyper_params`.\n",
    "- Create an empty list and assign to `mse_values`.\n",
    "- Use a `for` loop to iterate over `hyper_params` and in each iteration:\n",
    "    - Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        - `n_neighbors`: the current value for the iterator variable,\n",
    "        - `algorithm`: `brute`\n",
    "    - Fit the instantiated k-nearest neighbors model to the following columns from `train_df`:\n",
    "        - `accommodates`\n",
    "        - `bedrooms`\n",
    "        - `bathrooms`\n",
    "        - `number_of_reviews`\n",
    "    - Use the trained model to make predictions on the same columns from `test_df` and assign to `predictions`.\n",
    "    - Use the `mean_squared_error` function to calculate the MSE value between `predictions` and the `price` column from `test_df`.\n",
    "    - Append the MSE value to `mse_values`.\n",
    "- Display mse_values using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4db39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06caab56",
   "metadata": {},
   "source": [
    "## 3. Expanding grid search.\n",
    "\n",
    "Since our dataset is small and scikit-learn has been developed with performance in mind, the code ran quickly. As we increased the k value from 1 to 5, the MSE value fell from approximately 26364 to approximately 14090:\n",
    "\n",
    "| k | MSE                |\n",
    "|---|--------------------|\n",
    "| 1 | 26382.919226393627 |\n",
    "| 2 | 15145.281001137657 |\n",
    "| 3 | 14671.573884464668 |\n",
    "| 4 | 16513.548919226392 |\n",
    "| 5 | 14677.238680318545 |\n",
    "\n",
    "\n",
    "Let's expand grid search all the way to a k value of 20. While 20 may seem like an arbitrary ending point for our grid search, we can always expand the values we try if we're unconvinced that the lowest MSE value is associated with one of the hyperparameter values we tried so far.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Change the list of hyperparameter values, `hyper_params`, so it ranges from `1` to `20`.\n",
    "- Create an empty list and assign to `mse_values`.\n",
    "- Use a `for` loop to iterate over `hyper_params` and in each iteration:\n",
    "    - Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        - `n_neighbors`: the current value for the iterator variable,\n",
    "        - `algorithm`: `brute`\n",
    "    - Fit the instantiated k-nearest neighbors model to the following columns from `train_df`:\n",
    "        - `accommodates`\n",
    "        - `bedrooms`\n",
    "        - `bathrooms`\n",
    "        - `number_of_reviews`\n",
    "    - Use the trained model to make predictions on the same columns from `test_df` and assign to `predictions`.\n",
    "    - Use the `mean_squared_error` function to calculate the MSE value between `predictions` and the `price` column from `test_df`.\n",
    "    - Append the MSE value to `mse_values`.\n",
    "- Display `mse_values` using the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a08d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db2e080",
   "metadata": {},
   "source": [
    "## 4. Visualizing hyperparameter values.\n",
    "\n",
    "As we increased the `k` value from `1` to `6`, the MSE value decreased from approximately 26382 to approximately 13825. However, as we increased the k value from `7` to `20`, the MSE value didn't decrease further but instead hovered between approximately 14288 and 14870. This means that the optimal k value is `6`, since it resulted in the lowest MSE value.\n",
    "\n",
    "This pattern is something you'll notice while performing grid search across other models as well. As you increase k at first, the error rate decreases until a certain point, but then rebounds and increases again. Let's confirm this behavior visually using a scatter plot.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Use the `scatter()` method from `matplotlib.pyplot` to generate a line plot with:\n",
    "    - `hyper_params` on the x-axis,\n",
    "    - `mse_values` on the y-axis.\n",
    "- Use `plt.show()` to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2f80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6dd289",
   "metadata": {},
   "source": [
    "## 5. Varying Hyperparameters.\n",
    "\n",
    "From the scatter plot, you can tell that the lowest MSE value was achieved at the `k` value of `6`. As we increased `k` past `6`, the MSE actually increased and hovered but never decreased below 13657 (the approximate MSE value when `k` was `6`).\n",
    "\n",
    "Since varying the k value decreased the MSE value for this model, you may be wondering if repeating the grid search process for one of the models from the last lesson that performed poorly when we fixed k to `5` would result in a lower MSE value. Let's try it out!\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Use a `for` loop to iterate over `hyper_params` and in each iteration:\n",
    "    - Instantiate a KNeighborsRegressor object with the following parameters:\n",
    "        - `n_neighbors`: the current value for the iterator variable,\n",
    "        - `algorithm`: `brute`\n",
    "    - Fit the instantiated k-nearest neighbors model to all of the columns, except for the `price` column, from `train_df`\n",
    "    - Use the trained model to make predictions on the same columns from `test_df` and assign to `predictions`.\n",
    "    - Use the `mean_squared_error` function to calculate the MSE value between `predictions` and the price column from `test_df`.\n",
    "    - Append the MSE value to `mse_values`.\n",
    "- Use the `scatter()` method from `matplotlib.pyplot` to generate a line plot with:\n",
    "    - `hyper_params` on the x-axis,\n",
    "    - `mse_values` on the y-axis.\n",
    "- Use `plt.show()` to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b735c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18649095",
   "metadata": {},
   "source": [
    "## 6. Practice the workflow.\n",
    "\n",
    "You may have noticed that the general workflow for finding the best model is:\n",
    "\n",
    "select relevant features to use for predicting the target column.\n",
    "use grid search to find the optimal hyperparameter value for the selected features.\n",
    "evaluate the model's accuracy and repeat the process.\n",
    "Let's now practice this workflow.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- While using only the \"accommodates\" and \"bathrooms\" columns:\n",
    "    - Train a model for each k value between \"1\" and \"20\" using the training data.\n",
    "    - Use each model to make predictions on the test set (using just the \"accommodates\" and \"bathrooms\" columns).\n",
    "    - Calculate each model's MSE value by comparing each set of predictions to the true \"price\" values.\n",
    "    - Find the k value that obtained the lowest MSE value.\n",
    "    - Create a dictionary named \"two_hyp_mse\" that contains 1 key-value pair:\n",
    "        - key: k value that resulted in lowest MSE value.\n",
    "        - value: corresponding MSE value.\n",
    "- Repeat this process while using only the \"accommodates\", \"bathrooms\", and \"bedrooms\" columns:\n",
    "    - Create a dictionary named \"three_hyp_mse\" that contains 1 key-value pair:\n",
    "        - key: k value that resulted in lowest MSE value.\n",
    "        - value: corresponding MSE value.\n",
    "- Display both \"two_hyp_mse\" and \"three_hyp_mse\" using the \"print()\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06811db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
