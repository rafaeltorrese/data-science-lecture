{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9391ff93",
   "metadata": {},
   "source": [
    "<center><h1>Cross Validation</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9fdef",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "La validación de entrenamiento/prueba es una técnica simple para probar la precisión de un modelo de aprendizaje automático en datos nuevos en los que el modelo no fue entrenado. Ahora, nos centraremos en técnicas más sólidas.\n",
    "\n",
    "Para empezar, nos centraremos en la técnica de **validación por exclusión (holdout validation)**, que implica:\n",
    "\n",
    "- dividir el conjunto de datos completo en 2 particiones:\n",
    "    - un conjunto de entrenamiento\n",
    "    - un conjunto de prueba\n",
    "- entrenar el modelo en el conjunto de entrenamiento,\n",
    "- usar el modelo entrenado para predecir etiquetas en el conjunto de prueba,\n",
    "- calcular una métrica de error para comprender la efectividad del modelo,\n",
    "- cambiar los conjuntos de entrenamiento y prueba y repita,\n",
    "- promediar los errores.\n",
    "\n",
    "En la validación por exclusión, generalmente usamos una división 50/50 en lugar de la división 75/25 de la validación de entrenamiento/prueba. De esta forma, eliminamos el número de observaciones como fuente potencial de variación en el rendimiento de nuestro modelo.\n",
    "\n",
    "<img src=\"figs/holdout_validation.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "\n",
    "Comencemos por dividir el conjunto de datos en 2 mitades casi equivalentes.\n",
    "\n",
    "Cuando divida el conjunto de datos, no olvide configurar una copia usando `.copy()` para asegurarse de no obtener resultados inesperados más adelante. Si ejecuta el código localmente en Jupyter Notebook o Jupyter Lab sin `.copy()`, notará lo que se conoce como Advertencia de configuración con copia. Esto no evitará que su código se ejecute correctamente, pero le permite saber que cualquier operación que esté haciendo está tratando de establecerse en una copia de un segmento de un dataframe. Para asegurarse de que no vea esta advertencia, asegúrese de incluir `.copy()` cada vez que realice operaciones en un dataframe.\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Utilice la función `numpy.random.permutation()` para aleatorizar el orden de las filas en `dc_listings`.\n",
    "- Seleccione las primeras 1862 filas y asígnelas a `split_one`.\n",
    "- Seleccione las 1861 filas restantes y asígnelas a `split_two`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b7afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ec1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings = pd.read_csv(\"dc_airbnb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfcefb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc_listings['price'] = dc_listings['price'].str.replace('[\\$,]', '', regex=True).astype('float')\n",
    "\n",
    "print(len(dc_listings) / 2, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8de0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings = dc_listings.iloc[np.random.default_rng(2021).permutation(len(dc_listings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d563722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_one = dc_listings.iloc[:1862].copy()\n",
    "split_two = dc_listings.iloc[1862:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1af6b",
   "metadata": {},
   "source": [
    "## 2. Holdout Validation\n",
    "Ahora que hemos dividido nuestro conjunto de datos en 2 dataframes, hagamos lo siguiente:\n",
    "\n",
    "- entrenar un modelo de k-vecinos más cercanos en la primera mitad,\n",
    "- probar este modelo en la segunda mitad,\n",
    "- entrenar un modelo de k-vecinos más cercanos en la segunda mitad,\n",
    "- probar este modelo en la primera mitad.\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Entrene un modelo de k-vecinos más cercanos utilizando el algoritmo predeterminado (`auto`) y el número predeterminado de vecinos (`5`) que:\n",
    "     - Utiliza la columna `accommodates` de `train_one` para entrenar y\n",
    "     - Lo prueba en `test_one`.\n",
    "- Asigne el valor RMSE resultante a `iteration_one_rmse`.\n",
    "- Entrene un modelo de k-vecinos más cercanos utilizando el algoritmo predeterminado (`auto`) y el número predeterminado de vecinos (`5`) que:\n",
    "     - Utiliza la columna `accommodates` de `test_two` para entrenar y\n",
    "     - Lo prueba en `test_two`.\n",
    "- Asigne el valor RMSE resultante a `iteration_two_rmse`.\n",
    "- Use `numpy.mean()` para calcular el promedio de los 2 valores RMSE y asigne a `avg_rmse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af3dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one = split_one\n",
    "test_one = split_two\n",
    "train_two = split_two\n",
    "test_two = split_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d676940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsRegressor()\n",
    "knn1.fit(train_one[['accommodates']], train_one[['price']])\n",
    "predictions1 = knn1.predict(test_one[['accommodates']])\n",
    "iteration_one_rmse = np.sqrt(mean_squared_error(predictions1, test_one['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e1f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsRegressor()\n",
    "knn2.fit(train_two[['accommodates']], train_two['price'])\n",
    "predictions2 = knn2.predict(test_two[['accommodates']])\n",
    "iteration_two_rmse = np.sqrt(mean_squared_error(predictions2, test_two['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d7cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128.70692056945418\n",
      "125.33395047116689\n"
     ]
    }
   ],
   "source": [
    "print(iteration_one_rmse)\n",
    "print(iteration_two_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3deda56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.02043552031054\n"
     ]
    }
   ],
   "source": [
    "avg_rmse = np.mean([iteration_one_rmse, iteration_two_rmse])\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adce35",
   "metadata": {},
   "source": [
    "## 3. Validación cruzada K-Fold\n",
    "\n",
    "Si promediamos los dos valores RMSE del último paso, obtenemos un valor RMSE de aproximadamente 127.02. La validación por exclusión (holdout validation) es en realidad un ejemplo específico de una clase más grande de técnicas de validación llamada validación cruzada k-fold. \n",
    "\n",
    "Si bien la validación de exclusión es mejor que la validación de entrenamiento/prueba porque el modelo no está sesgado repetidamente hacia un subconjunto específico de datos, ambos modelos entrenados solo usan la mitad de los datos disponibles. La validación cruzada K-fold, por otro lado, aprovecha una mayor proporción de los datos durante el entrenamiento mientras sigue rotando a través de diferentes subconjuntos de datos para evitar los problemas de validación de entrenamiento/prueba.\n",
    "\n",
    "Aquí está el algoritmo de la validación cruzada de k-fold:\n",
    "\n",
    "- dividir el conjunto de datos completo en `k` particiones de igual longitud.\n",
    "    - seleccionando las `k-1` particiones  como el conjunto de entrenamiento y\n",
    "    - seleccionando la partición restante como el conjunto de prueba\n",
    "- entrenar el modelo en el conjunto de entrenamiento.\n",
    "- usar el modelo entrenado para predecir etiquetas en el conjunto de prueba.\n",
    "- calcular la métrica de error del conjunto prueba.\n",
    "- repetir todos los pasos anteriores `k-1` veces, hasta que cada partición se haya utilizado como conjunto de prueba para una iteración.\n",
    "- calcular la media de los valores de error `k`.\n",
    "\n",
    "La validación de exclusión es esencialmente una versión de la validación cruzada k-fold cuando `k` es igual a `2`. En general, los pliegues (número de subconjuntos) de `5` o `10` se utilizan para la validación cruzada k-fold. Aquí hay un diagrama que describe cada iteración de la validación cruzada de 5 subconjuntos:\n",
    "\n",
    "<img src=\"figs/kfold_cross_validation.png\" width=\"800\" height=\"600\" />\n",
    "\n",
    "A medida que aumenta el número de pliegues, el número de observaciones en cada pliegue (subconjunto) disminuye y la varianza de los errores entre cada pliegue aumenta. \n",
    "\n",
    "Comencemos dividiendo manualmente el conjunto de datos en 5 pliegues. En lugar de dividir en 5 dataframes, agreguemos una columna que especifique a qué subconjunto pertenece la fila. De esta manera, podemos seleccionar fácilmente nuestro conjunto de entrenamiento y nuestro conjunto de prueba.\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Agregue una nueva columna a `dc_listings` llamada `fold` que contiene el número de subconjunto al que pertenece cada fila:\n",
    "- El pliegue `1` debe tener filas desde el índice `0` hasta `745`, sin incluir `745`.\n",
    "- El pliegue `2` debe tener filas desde el índice `745` hasta `1490`, sin incluir `1490`.\n",
    "- El pliegue `3` debe tener filas desde el índice `1490` hasta `2234`, sin incluir `2234`.\n",
    "- El pliegue `4` debe tener filas desde el índice `2234` hasta `2978`, sin incluir `2978`.\n",
    "- El pliegue `5` debe tener filas desde el índice `2978` hasta `3723`, sin incluir `3723`.\n",
    "- Asegúrate de que el tipo de `fold` sea un tipo flotante.\n",
    "- Cuente los valores únicos para la columna `fold` para confirmar que cada pliegue tiene aproximadamente la misma cantidad de elementos.\n",
    "- Muestra el número de valores faltantes (missing values) en la columna `fold` para confirmar que no nos falta ninguna fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8eec355",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings.loc[dc_listings.index[:745], 'fold'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd16188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604     1.0\n",
       "2493    1.0\n",
       "904     1.0\n",
       "1935    1.0\n",
       "544     1.0\n",
       "       ... \n",
       "2127    NaN\n",
       "3388    NaN\n",
       "1769    NaN\n",
       "1333    NaN\n",
       "3412    NaN\n",
       "Name: fold, Length: 3723, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_listings['fold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2375faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_listings.loc[dc_listings.index[745:1490], 'fold'] = 2\n",
    "dc_listings.loc[dc_listings.index[1490:2234], 'fold'] = 3\n",
    "dc_listings.loc[dc_listings.index[2234:2978], 'fold'] = 4\n",
    "dc_listings.loc[dc_listings.index[2978:], 'fold'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0503b59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    745\n",
       "2.0    745\n",
       "3.0    744\n",
       "4.0    744\n",
       "5.0    745\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_listings['fold'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83bb8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_listings['fold'] = 1.0\n",
    "# dc_listings.fold.iloc[745:1490] = 2\n",
    "# dc_listings.fold.iloc[1490:2234] = 3\n",
    "# dc_listings.fold.iloc[2234:2978] = 4\n",
    "# dc_listings.fold.iloc[2978:] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba851bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dc_listings['fold'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eadce7",
   "metadata": {},
   "source": [
    "## 4. Primera iteración\n",
    "\n",
    "Comencemos por realizar la primera iteración de la validación cruzada de k-fold en un modelo univariado.\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Entrene un modelo de k-vecinos más cercanos usando la columna `accommodates` como la única característica de los pliegues `2` a `5` como conjunto de entrenamiento.\n",
    "- Use el modelo para hacer predicciones en el conjunto de prueba (columna `accommodates` del subconjunto `1`) y asigne las etiquetas pronosticadas a `labels`.\n",
    "- Calcule el valor RMSE comparando la columna `price` con las etiquetas pronosticadas.\n",
    "- Asigne el valor RMSE a `iteration_one_rmse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce8d8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(dc_listings[dc_listings['fold'] != 1]['fold'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8007c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_kfold(k, mode='exclude'):\n",
    "    if mode == 'exclude':\n",
    "        return dc_listings[dc_listings['fold'] != k].copy()\n",
    "    elif mode == 'include':\n",
    "        return dc_listings[dc_listings['fold'] == k].copy()\n",
    "    raise Exception(\"Write mode parameter as 'exclude' or 'include'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6009afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = select_kfold(1, mode='exclude')\n",
    "test_df = select_kfold(1, mode='include')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330bf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(training_df[['accommodates']], training_df['price'])\n",
    "labels = knn.predict(test_df[['accommodates']])\n",
    "iteration_one_rmse = np.sqrt(mean_squared_error(labels, test_df['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4130b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.45677104635922\n"
     ]
    }
   ],
   "source": [
    "print(iteration_one_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3cb44",
   "metadata": {},
   "source": [
    "## 5. Función para entrenar modelos\n",
    "\n",
    "Desde la primera iteración, logramos un valor RMSE de aproximadamente **111**. Calculemos los valores RMSE para las iteraciones restantes. Para facilitar el proceso de iteración, coloquemos el código que escribimos anteriormente en una función.\n",
    "\n",
    "\n",
    "### Ejercicio\n",
    "\n",
    "- Escriba una función llamada `train_and_validate` que tome un dataframe como primer parámetro (`df`) y una lista de valores de subconjuntos (`1` a `5` en nuestro caso) como segundo parámetro (`folds`). Esta función debería:\n",
    "    - Entrenar modelos `n` (donde `n` es el número de subconjuntos) y realizar una validación cruzada k-fold (usando `n` subconjuntos). Utilice el valor `k` predeterminado para la clase `KNeighborsRegressor`.\n",
    "    - Devuelva una lista de valores RMSE, donde el primer elemento es el RMSE cuando el subconjunto `1` fue el conjunto de prueba, el segundo elemento es el RMSE para cuando el subconjunto `2` fue el conjunto de prueba, y así sucesivamente.\n",
    "- Use la función `train_and_validate` para devolver la lista de valores RMSE para el dataframe `dc_listings` y asigne a `rmses`.\n",
    "- Calcular la media de estos valores y asignar a `avg_rmse`.\n",
    "- Mostrar tanto `rmses` como `avg_rmse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(df, folds):\n",
    "    rmses = []\n",
    "    \n",
    "    for n in folds:\n",
    "        training_df = df[df['fold'] != n].copy()\n",
    "        test_df = df[df['fold'] == n].copy()\n",
    "\n",
    "        knn = KNeighborsRegressor() \n",
    "        knn.fit(training_df[['accommodates']], training_df['price'])\n",
    "\n",
    "        labels = knn.predict(test_df[['accommodates']])\n",
    "        rmses.append(np.sqrt(mean_squared_error(labels, test_df['price'])))\n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c17e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = train_and_validate(dc_listings, folds=[1, 2, 3, 4, 5])\n",
    "avg_rmse = np.mean(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c25a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmses)\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113116e",
   "metadata": {},
   "source": [
    "## 6. Performing K-Fold Cross Validation Using Scikit-Learn\n",
    "\n",
    "While the average RMSE value was approximately 135, the RMSE values ranged from 102 to 159+. This large amount of variability between the RMSE values means that we're either using a poor model or a poor evaluation criteria (or a bit of both!). By implementing your own k-fold cross-validation function, you hopefully acquired a good understanding of the inner workings of the technique. The function we wrote, however, has many limitations. If we want to now change the number of folds we want to use, we need to make the function more general so it can also handle randomizing the ordering of the rows in the dataframe and splitting into folds.\n",
    "\n",
    "In machine learning, we're interested in building a good model and accurately understanding how well it will perform. To build a better k-nearest neighbors model, we can change the features it uses or tweak the number of neighbors (a hyperparameter). To accurately understand a model's performance, we can perform k-fold cross validation and select the proper number of folds. We've learned how scikit-learn makes it easy for us to quickly experiment with these different knobs when it comes to building a better model. Let's now dive into how we can use scikit-learn to handle cross-validation as well.\n",
    "\n",
    "First, we instantiate an instance of the [KFold class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) from `sklearn.model_selection`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits, shuffle=False, random_state=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `n_splits` is the number of folds you want to use,\n",
    "- `shuffle` is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "- `random_state` is used to specify the random seed value if `shuffle` is set to `True`.\n",
    "\n",
    "You'll notice here that no parameters depend on the data set at all. This is because the KFold class returns an iterator object which we use in conjunction with the [cross_val_score()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, also from `sklearn.model_selection`. Together, these 2 functions allow us to compactly train and test using k-fold cross validation:\n",
    "\n",
    "Here are the relevant parameters for the `cross_val_score` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "- `estimator` is a sklearn model that implements the `fit` method (e.g. instance of KNeighborsRegressor),\n",
    "- `X` is the list or 2D array containing the features you want to train on,\n",
    "- `y` is a list containing the values you want to predict (target column),\n",
    "- `scoring` is a string describing the scoring criteria (list of accepted values here).\n",
    "- `cv` describes the number of folds. Here are some examples of accepted values:\n",
    "    - an instance of the `KFold` class,\n",
    "    - an integer representing the number of folds.\n",
    "\n",
    "Depending on the scoring criteria you specify, a single total value is returned for each fold. Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "\n",
    "- instantiate the scikit-learn model class you want to fit,\n",
    "- instantiate the `KFold` class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "- use the `cross_val_score()` function to return the scoring metric you're interested in.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Create a new instance of the `KFold` class with the following properties:\n",
    "    - `5` folds,\n",
    "    - shuffle set to `True`,\n",
    "    - random seed set to `1` (so we can answer check using the same seed),\n",
    "    - assigned to the variable `kf`.\n",
    "- Create a new instance of the `KNeighborsRegressor` class and assign to `knn`.\n",
    "- Use the `cross_val_score()` function to perform k-fold cross-validation:\n",
    "    - using the KNeighborsRegressor instance `knn`,\n",
    "    - using the `accommodates` column for training,\n",
    "    - using the `price` column as the target column,\n",
    "    - using the string `neg_mean_squared_error` as the value of the `scoring` parameter,\n",
    "    - using `kf` as the value of the `cv` parameter\n",
    "    - returning an array of MSE values (one value for each fold).\n",
    "- Assign the resulting list of MSE values to `mses`. Then, take the absolute value followed by the square root of each MSE value. Then, calculate the average of the resulting RMSE values and assign to `avg_rmse`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = cross_val_score(estimator=knn, \n",
    "                       X=dc_listings[['accommodates']], \n",
    "                       y=dc_listings['price'], \n",
    "                       scoring='neg_mean_squared_error', \n",
    "                       cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rmse = np.sqrt(np.abs(mses)).mean()\n",
    "print(avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d36d507",
   "metadata": {},
   "source": [
    "## 7. Exploring Different K Values\n",
    "Choosing the right `k` value when performing k-fold cross validation is more of an art and less of a science. As we discussed earlier in the lesson, a `k` value of `2` is really just holdout validation. On the other end, setting `k` equal to `n` (the number of observations in the data set) is known as **leave-one-out cross validation**, or LOOCV for short. Through lots of trial and error, data scientists have converged on `10` as the standard k value.\n",
    "\n",
    "In the following code block, we display the results of varying `k` from `3` to `23`. For each `k` value, we calculate and display the average RMSE value across all of the folds and the standard deviation of the RMSE values. Across the many different `k` values, it seems like the average RMSE value is around `129`. You'll notice that the standard deviation of the RMSE increases from approximately `8` to over `40` as we increase the number of folds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]\n",
    "\n",
    "for fold in num_folds:\n",
    "    kf = KFold(fold, shuffle=True, random_state=1)\n",
    "    model = KNeighborsRegressor()\n",
    "    mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    rmses = np.sqrt(np.absolute(mses))\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    std_rmse = np.std(rmses)\n",
    "    print(f'{fold} folds: avg RMSE: {avg_rmse}, std RMSE: {std_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e27f3",
   "metadata": {},
   "source": [
    "## 8. Bias-Variance Tradeoff\n",
    "\n",
    "So far, we've been working under the assumption that a lower RMSE always means that a model is more accurate. This isn't the complete picture, unfortunately. A model has two sources of error, **bias** and **variance**.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance. In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "The standard deviation of the RMSE values can be a proxy for a model's **variance** while the average RMSE is a proxy for a model's **bias**. Bias and variance are the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "\n",
    "<img src=\"figs/bias_variance.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "While k-nearest neighbors can make predictions, it isn't a mathematical model. A mathematical model is usually an equation that can exist without the original data, which isn't true with k-nearest neighbors. In the next two courses, we'll learn about a mathematical model called linear regression. We'll explore the bias-variance tradeoff in greater depth in these next 2 courses because of its importance when working with mathematical models in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4f201",
   "metadata": {},
   "source": [
    "## 9. Next steps\n",
    "\n",
    "In this lesson, we explored more robust cross validation techniques like holdout validation and k-fold cross-validation. Next in this course is a guided project where you can practice what you've learned in this course on a different data set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
